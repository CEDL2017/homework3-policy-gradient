{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve a classic control problem - CartPole using policy gradient methods.\n",
    "\n",
    "Policy gradient method is a family of RL algorithms that parameterizes the policy directly. The simplest advantage that policy parameterization may have over action-value parameterization is that the policy may be a simpler function to approximate.\n",
    "\n",
    "First, you will implement the \"vanilla\" policy gradient method, i.e., a method that repeatedly computes **unbiased** estimates $\\hat{g}$ of $\\nabla_{\\theta} E[\\sum_t r_t]$ and takes gradient ascent steps $\\theta \\rightarrow \\theta + \\epsilon \\hat{g}$ so as to increase the total rewards collected in each episode. To make sure our code can solve multiple MDPs with different policy parameterizations, provided code follows an OOP manner and represents MDP and Policy as classes.\n",
    "\n",
    "The following code constructs an instance of the MDP using OpenAI gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from policy_gradient import util\n",
    "from policy_gradient.policy import CategoricalPolicy\n",
    "from policy_gradient.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# CartPole-v0 is a MDP with finite state and action space. \n",
    "# In this environment, A pendulum is attached by an un-actuated joint to a cart, \n",
    "# and the goal is to prevent it from falling over. You can apply a force of +1 or -1 to the cart.\n",
    "# A reward of +1 is provided for every timestep that the pendulum remains upright. \n",
    "# To visualize CartPole-v0, please see https://gym.openai.com/envs/CartPole-v0\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: construct a neural network to represent policy\n",
    "\n",
    "Make sure you know how to construct neural network using tensorflow.\n",
    "\n",
    "1. Open **homework2/policy_gradient/policy.py**.\n",
    "2. Follow the instruction of Problem 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: compute the surrogate loss\n",
    "\n",
    "If there are $N$ episodes in an iteration, then for $i$ th episode we define $R_t^i = \\sum_{{t^′}=t}^T \\gamma^{{t^′}-t}r(s_{t^′}, a_{t^′})$ as the accumulated discounted rewards from timestep $t$ to the end of that episode, where $\\gamma$ is the discount rate.\n",
    "\n",
    "The pseudocode for the REINFORCE algorithm is as below:\n",
    "\n",
    "1. Initialize policy $\\pi$ with parameter $\\theta_1$.\n",
    "2. For iteration $k = 1, 2, ...$:\n",
    "    * Sample N episodes $\\tau_1, \\tau_2, ..., \\tau_N$ under the current policy $\\theta_k$, where $\\tau_i =(s_i^t,a_i^t,R_i^t)_{t=0}^{T−1}$. Note that the last state is dropped since no action is taken after observing the last state.\n",
    "    * Compute the empirical policy gradient using formula: $$\\hat{g} = E_{\\pi_\\theta}[\\nabla_{\\theta} log\\pi_\\theta(a_t^i | s_t^i) R_t^i]$$\n",
    "    * Take a gradient step: $\\theta_{k+1} = \\theta_k + \\epsilon \\hat{g}$.\n",
    "    \n",
    "    \n",
    "Note that we can transform the policy gradient formula as\n",
    "\n",
    "$$\\hat{g} = \\nabla_{\\theta} \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) *R_t^i)$$\n",
    "\n",
    "and $L(\\theta) = \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) *R_t^i)$ is called the surrogate loss. \n",
    "\n",
    "We can first construct the computation graph for $L(\\theta)$, and then take its gradient as the empirical policy gradient.\n",
    "\n",
    "\n",
    "1. Open **homework2/policy_gradient/policy.py**.\n",
    "2. Follow the instruction of Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "# Construct a neural network to represent policy which maps observed state to action. \n",
    "in_dim = util.flatten_space(env.observation_space)\n",
    "out_dim = util.flatten_space(env.action_space)\n",
    "hidden_dim = 8\n",
    "\n",
    "# Initialize your policy\n",
    "with tf.variable_scope(\"policy\"):\n",
    "    opt_p = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    policy = CategoricalPolicy(in_dim, out_dim, hidden_dim, opt_p, sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Use baseline to reduce the variance of our gradient estimate.\n",
    "\n",
    "Change the loss term: \n",
    "\n",
    "$$L(\\theta) = \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) *R_t^i)$$\n",
    "\n",
    "into this one\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{(NT)}(\\sum_{i=1}^N \\sum_{t=0}^T log\\pi_\\theta(a_t^i | s_t^i) *(R_t^i-V_t^i))$$\n",
    "\n",
    "where $V_t^i$ is the baseline prediction for at the $t$ timestep $i^{th}$ trajectory. In this part, we use a simple linear function to parameterize value function. (You're encouraged to see the details in ```policy_gradient/baselines/linear_feature_baseline.py```)\n",
    "\n",
    "1. Fill in the function `process_paths` of class `PolicyOptimizer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyOptimizer(object):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.env = env\n",
    "        self.n_iter = n_iter\n",
    "        self.n_episode = n_episode\n",
    "        self.path_length = path_length\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "    def sample_path(self):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        ob = self.env.reset()\n",
    "\n",
    "        # sample a batch of trajectory\n",
    "        for _ in range(self.path_length):\n",
    "            a = self.policy.act(ob.reshape(1, -1))\n",
    "            next_ob, r, done, _ = self.env.step(a)\n",
    "            obs.append(ob)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            ob = next_ob\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return dict(\n",
    "            observations=np.array(obs),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "        )\n",
    "\n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "                b[-1] = 0 # terminal state\n",
    "            else:\n",
    "                b = 0\n",
    "            \n",
    "            # `p[\"rewards\"]` is a matrix contains the rewards of each timestep in a sample path\n",
    "            r = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "            \n",
    "            \"\"\"\n",
    "            Problem 3:\n",
    "\n",
    "            1. Variable `b` is the values predicted by our baseline\n",
    "            2. Use it to reduce variance and then assign the result to the \n",
    "                    variable `a` (baseline reduction)\n",
    "\n",
    "            Sample solution should be only 1 line.\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE >>>>>>\n",
    "            \n",
    "            a = r - b\n",
    "            \n",
    "            # <<<<<<<<\n",
    "\n",
    "            p[\"returns\"] = r\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        loss_list = []\n",
    "        avg_return_list = []\n",
    "        for i in range(1, self.n_iter + 1):\n",
    "            paths = []\n",
    "            for _ in range(self.n_episode):\n",
    "                paths.append(self.sample_path())\n",
    "            data = self.process_paths(paths)\n",
    "            loss = self.policy.train(data[\"observations\"], data[\"actions\"], data[\"advantages\"])\n",
    "            avg_return = np.mean([sum(p[\"rewards\"]) for p in paths])\n",
    "            print(\"Iteration {}: Average Return = {}\".format(i, avg_return))\n",
    "            loss_list.append(loss)\n",
    "            avg_return_list.append(avg_return)\n",
    "            # CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials.\n",
    "            if avg_return >= 195:\n",
    "                print(\"Solve at {} iterations, which equals {} episodes.\".format(i, i*100))\n",
    "                break\n",
    "\n",
    "            if self.baseline != None:\n",
    "                self.baseline.fit(paths)\n",
    "        return loss_list, avg_return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "baseline = LinearFeatureBaseline(env.spec)\n",
    "\n",
    "po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "loss_list, avg_return_list = po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_curve(loss_list, \"loss\")\n",
    "util.plot_curve(avg_return_list, \"average return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify your solutions\n",
    "\n",
    "if you solve the problems 1~3 correctly, your will solve CartPole with roughly ~ 80 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Replacing line \n",
    "\n",
    "`baseline = LinearFeatureBaseline(env.spec)` \n",
    "\n",
    "with \n",
    "\n",
    "`baseline = None`\n",
    "\n",
    "can remove the baseline.\n",
    "\n",
    "Modify the code to compare the variance and performance before and after adding baseline. And explain wht the baseline won't introduce bias. Then, write a report about your findings and explainations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "baseline = None\n",
    "\n",
    "po = PolicyOptimizer(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "loss_list, avg_return_list = po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_curve(loss_list, \"loss\")\n",
    "util.plot_curve(avg_return_list, \"average return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Actor-Critic algorithm (with bootstrapping)\n",
    "\n",
    "The baseline reduction lowers the variance without the cost of introducing bias. Are there any others that can reduce the variance of the policy?\n",
    "\n",
    "In this part, you will implement a simple actor-critic algorithm. Try to overwrite the `process_paths` in `PolicyOptimizer_actor_critic`. Note that the `PolicyOptimizer_actor_critic` inherit the class `PolicyOptimizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use the one-step bootstrap for the advantage function, which change the $A_t^i=(R_t^i-V_t^i)$ in problem 3 into:\n",
    "\n",
    "$$A_t^i = r_t^i + \\gamma*V_{t+1}^i - V_t^i$$\n",
    "\n",
    "Open **policy_gradient/util.py ** to implement the function `discount_bootstrap`\n",
    "\n",
    "If you answer is right, your will solve CartPole with roughly ~ 80 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the hyperparameter for generalized advantage estimation (GAE)\n",
    "LAMBDA = 0.98 # \\lambda\n",
    "class PolicyOptimizer_actor_critic(PolicyOptimizer):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "        PolicyOptimizer.__init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "            discount_rate=.99)\n",
    "        \n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "                b[-1] = 0 # terminal state\n",
    "            else:\n",
    "                b = 0\n",
    "            \n",
    "            \"\"\"\n",
    "            1. Variable `b` is the reward predicted by our baseline\n",
    "            2. Calculate the advantage function via one-step bootstrap\n",
    "                    A(s, a) = [r(s,a,s') + \\gamma*v(s')] - v(s)\n",
    "            3. `target_v` specifies the target of the baseline function\n",
    "            \"\"\"\n",
    "            r = util.discount_bootstrap(p[\"rewards\"], self.discount_rate, b)\n",
    "            target_v = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "            a = r - b\n",
    "\n",
    "            p[\"returns\"] = target_v\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "# reinitialize the baseline function\n",
    "baseline = LinearFeatureBaseline(env.spec) \n",
    "sess.run(tf.global_variables_initializer())\n",
    "po = PolicyOptimizer_actor_critic(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "loss_list, avg_return_list = po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_curve(loss_list, \"loss\")\n",
    "util.plot_curve(avg_return_list, \"average return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Generalized Advantage Estimation\n",
    "\n",
    "In Problem 3, we calculate the advantage function with **the total return substract the baseline** \n",
    "\n",
    "In Problem 5, we calculate the advantage function based on Problem 3, and **replace the total return with the immediate reward plus estimated baseline**\n",
    "\n",
    "Here, we use a novel advantage function called \"Generalized Advantage Estimation\", which introduces one hyperparameter $\\lambda$ to compromise the above two estimation methods.\n",
    "\n",
    "Assume the $\\delta_t^i$ represent the i-step bootstrapping (e.g. $\\delta_t^i=r_t^i + \\gamma*V_{t+1}^i - V_t^i$). The generalized advantage estimation will be:\n",
    "\n",
    "$$A_{t}^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+1}$$\n",
    "\n",
    "Please see [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) for details.\n",
    "\n",
    "\n",
    "If you answer is right, your will solve CartPole with roughly ~ 80 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the hyperparameter for generalized advantage estimation (GAE)\n",
    "LAMBDA = 0.98 # \\lambda\n",
    "class PolicyOptimizer_actor_critic(PolicyOptimizer):\n",
    "    def __init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "        discount_rate=.99):\n",
    "        '''\n",
    "        PolicyOptimizer.__init__(self, env, policy, baseline, n_iter, n_episode, path_length,\n",
    "            discount_rate=.99)\n",
    "        '''\n",
    "        super(PolicyOptimizer_actor_critic, self).__init__(env, policy, baseline, n_iter, n_episode, path_length, discount_rate=.99)\n",
    "    \n",
    "    def process_paths(self, paths):\n",
    "        for p in paths:\n",
    "            if self.baseline != None:\n",
    "                b = self.baseline.predict(p)\n",
    "                b[-1] = 0 # terminal state\n",
    "            else:\n",
    "                b = 0\n",
    "            \n",
    "            \"\"\"\n",
    "            1. Variable `b` is the reward predicted by our baseline\n",
    "            2. Calculate the advantage function via one-step bootstrap\n",
    "                    A(s, a) = [r(s,a,s') + \\gamma*v(s')] - v(s)\n",
    "            3. `target_v` specifies the target of the baseline function\n",
    "            \"\"\"\n",
    "            r = util.discount_bootstrap(p[\"rewards\"], self.discount_rate, b)\n",
    "            target_v = util.discount_cumsum(p[\"rewards\"], self.discount_rate)\n",
    "            a = r - b\n",
    "            \n",
    "            \"\"\"\n",
    "            Problem 6 (Skip this when doing Proble): \n",
    "            \n",
    "            1. in Problem 3, we calculate the advantage function with the total \n",
    "                       return substract the baseline \n",
    "               in Problem 5, we calculate the advantage function based on Problem 3. \n",
    "                       And replace the total return with the immediate reward plus estimated baseline\n",
    "            2. Here, we use a novel advantage function called \"Generalized Advantage Estimation\", \n",
    "                       which introduces one hyperparameter to balance the above two estimation methods.\n",
    "            Currently, the variable `LAMBDA` is the default hyperparameter for GAE. \n",
    "                        (You're encouraged to try other \\lambda=[0,1])\n",
    "            Sample solution should be only 1 line. (you can use `util.discount` in policy_gradient/util.py)\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE >>>>>>>>\n",
    "            \n",
    "            a = util.discount(a, self.discount_rate * LAMBDA)\n",
    "            \n",
    "            # <<<<<<<\n",
    "            p[\"returns\"] = target_v\n",
    "            p[\"baselines\"] = b\n",
    "            p[\"advantages\"] = (a - a.mean()) / (a.std() + 1e-8) # normalize\n",
    "\n",
    "        obs = np.concatenate([ p[\"observations\"] for p in paths ])\n",
    "        actions = np.concatenate([ p[\"actions\"] for p in paths ])\n",
    "        rewards = np.concatenate([ p[\"rewards\"] for p in paths ])\n",
    "        advantages = np.concatenate([ p[\"advantages\"] for p in paths ])\n",
    "\n",
    "        return dict(\n",
    "            observations=obs,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            advantages=advantages,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "n_iter = 200\n",
    "n_episode = 100\n",
    "path_length = 200\n",
    "discount_rate = 0.99\n",
    "# reinitialize the baseline function\n",
    "baseline = LinearFeatureBaseline(env.spec) \n",
    "sess.run(tf.global_variables_initializer())\n",
    "po = PolicyOptimizer_actor_critic(env, policy, baseline, n_iter, n_episode, path_length,\n",
    "                     discount_rate)\n",
    "\n",
    "# Train the policy optimizer\n",
    "loss_list, avg_return_list = po.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_curve(loss_list, \"loss\")\n",
    "util.plot_curve(avg_return_list, \"average return\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
